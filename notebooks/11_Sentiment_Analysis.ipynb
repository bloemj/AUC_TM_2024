{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Today\n",
    ">\n",
    ">- [An Introduction to Sentiment Analysis](#An-Introduction-to-Sentiment-Analysis)\n",
    ">\n",
    ">\n",
    ">- [Sentiment Classification](#Sentiment-Classification)\n",
    ">\n",
    ">\n",
    ">- [Machine Learning Approaches](#Machine-Learning-Approaches)\n",
    ">\n",
    ">\n",
    ">- [Lexicon-based Approaches](#Lexicon-based-Approaches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "from sklearn import naive_bayes, metrics\n",
    "\n",
    "# plt.rcdefaults()\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Introduction to Sentiment Analysis\n",
    "\n",
    "\n",
    "**Sentiment Analysis** (also called Opinion Mining) is the computational study of people's opinions, attitudes and emotions toward an entity (a person, an object, an event, a topic) expressed in a text.\n",
    "\n",
    "- SA is difficult because it is often:\n",
    "\n",
    "    - necessary to include word knowledge;\n",
    "\n",
    "    - necessary to have some (basic) model of how emotions work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Sentiment Analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Business and Organizations**: how do consumers feel about a product and its features? Where is the market going?\n",
    "\n",
    "\n",
    "\n",
    "- **Ad placement**: show an ad for product $x$ to users who expressed positive sentiments on products similar to $x$, e.g. in social media posts.\n",
    "\n",
    "\n",
    "\n",
    "- **Individuals**: what product may a user like/dislike? What other users share similar feelings towards a given entity?\n",
    "\n",
    "\n",
    "\n",
    "- **Politics**: what to people think about a candidate or an issue? What is going to be the outcome of an election?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Credits**: The following section is based on Liu (2010) Sentiment Analysis and Subjectivity. In Indurkhya & Damerau (eds.) *Handbook of Natural Language Processing*, CRC Press: Chapter 26."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1). *I bought an iPhone a few days ago.* \n",
    "    - a fact, not an opinion\n",
    "\n",
    "\n",
    "(2). *It was such a nice phone.*\n",
    "    - positive opinion\n",
    "    - on the phone as a whole\n",
    "    - from the author of the review\n",
    "\n",
    "\n",
    "(3). *The touch screen was really cool.*\n",
    "    - positive opinion\n",
    "    - on a component of the phone\n",
    "    - from the author of the review\n",
    "\n",
    "\n",
    "(4). *The voice quality was clear too.* \n",
    "    - positive opinion\n",
    "    - on a characteristic of the phone\n",
    "    - from the author of the review\n",
    "\n",
    "\n",
    "(5). *Although the battery life was not long, that is ok for me.*\n",
    "    - negative opinion?\n",
    "    - on another characteristic of the phone\n",
    "    - from the author of the review\n",
    "\n",
    "\n",
    "(6). *However, my mother was mad with me as I did not tell her before I bought it.*\n",
    "    - negative emotion/opinion\n",
    "    - towards the author of the review\n",
    "    - from the mother of the author of the review\n",
    "\n",
    "\n",
    "(7). *She also thought the phone was too expensive, and wanted me to return it to the shop.*\n",
    "    - negative opinion\n",
    "    - on a characteristic of the phone\n",
    "    - from the mother of the author of the review\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition of Objects**:\n",
    "\n",
    "An object $o$ is an entity that can be a product, person, event, organization, or topic. It is associated with a pair $o : (T, A)$, where $T$ is a hierarchy of components (or parts), subcomponents, and so on, and $A$ is a set of attributes of $o$. Each component has its own set of subcomponents and attributes.\n",
    "\n",
    "An opinion can be expressed on the object as a whole, on one or several of its components, or on one or several of the attributes.\n",
    "\n",
    "> **Example**:\n",
    ">\n",
    ">A particular cell phone model by some phone manufacturer is an object. It has a set of components, e.g., *battery*, and *screen*, and also a set of attributes, e.g., *voice quality*, *size*, and *weight*. The battery component also has its set of attributes, e.g. *battery life* and *battery replacement ease*.\n",
    "\n",
    "Often, the distinction between attributes and components is ignored in order to facilitate the task, and both concepts are referred to simply as **object features**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition of Opinion**:\n",
    "\n",
    "An opinion on a feature f is a positive or negative view, attitude, emotion, or appraisal on f from an opinion holder:\n",
    "\n",
    "\n",
    "- **polarity** (a.k.a.) opinion orientation: The *orientation* of an opinion on a feature f indicates whether the opinion is positive, negative, or neutral.\n",
    "\n",
    "\n",
    "- **emotion**: *subjective feelings and thoughts*. Over the years, many [categorizations of emotions](https://en.wikipedia.org/wiki/Contrasting_and_categorization_of_emotions) have been proposed, e.g., Plutchik's \"8 basic emotions\" (acceptance, anger, anticipation, disgust, joy, fear, sadness, surprise).\n",
    "\n",
    "\n",
    "- **opinion holder**: *the person or organization that expresses the opinion*\n",
    "    - in the case of product reviews and blogs, opinion holders are usually the authors of the posts;\n",
    "    - sometimes (e.g., in a news article, in example (6) above), the person or organization that holds a particular opinion is explicitely mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Text Mining Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Sentiment Classification**: classify the opinions expressed in a text or a document as mostly positive, negative or neutral.\n",
    "    - Opinions can be measured on different scales, e.g., {-1, 0, 1} or 5 star ratings.\n",
    "    \n",
    "\n",
    "\n",
    "- **Subjectivity Classification**: detect if a text contains only factual content or if it expresses some personal feelings, beliefs, emotions, evaluations, opinions.\n",
    "\n",
    "\n",
    "\n",
    "- **Opinion Summarization**: summarize the opinions generated by describing a specific entity.\n",
    "    - Feature-based opinion summarization: summarize the opinions for the main evaluated aspects of a given entity.\n",
    "    - Single-document vs. multi-document.\n",
    "\n",
    "\n",
    "\n",
    "- **Opinion Retrieval**: retrieve documents that expresses an opinion about an entity of interest (and/or about some of its characteristics).\n",
    "\n",
    "\n",
    "\n",
    "- **Emotion Detection**: extract the emotions that are implicitly or explititly express in a text.\n",
    "\n",
    "\n",
    "\n",
    "- **Sarcasm/Irony Detection**: identify ironic or sarcastic statements.\n",
    "    - NB: it is difficult to define irony and/or sarcasm.\n",
    "\n",
    "\n",
    "\n",
    "- **Opinion Spam Detection**: identify messages or reviews which contains contents whose main intent is to distort the public perception of an entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification\n",
    "\n",
    "In general it is approached as **the classification the opinions expressed in a text or a document as mostly positive, negative or neutral**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [Methad et al. (2014)](http://www.sciencedirect.com/science/article/pii/S2090447914000550)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercises we will experiment with the Movie Reviews in the Sentiment Polarity Dataset by Pang & Lee (2004):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Polarity Dataset Version 2.0\n",
      "Bo Pang and Lillian Lee\n",
      "\n",
      "http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
      "\n",
      "Distributed with NLTK with permission from the authors.\n",
      "\n",
      "=======\n",
      "\n",
      "Introduction\n",
      "\n",
      "This README v2.0 (June, 2004) for the v2.0 polarity dataset comes from\n",
      "the URL http://www.cs.cornell.edu/people/pabo/movie-review-data .\n",
      "\n",
      "=======\n",
      "\n",
      "What's New -- June, 2004\n",
      "\n",
      "This dataset represents an enhancement of the review corpus v1.0\n",
      "described in README v1.1: it contains more reviews, and labels were\n",
      "created with an improved rating-extraction system.\n",
      "\n",
      "=======\n",
      "\n",
      "Citation Info \n",
      "\n",
      "This data was first used in Bo Pang and Lillian Lee,\n",
      "``A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization \n",
      "Based on Minimum Cuts'',  Proceedings of the ACL, 2004.\n",
      "\n",
      "@InProceedings{Pang+Lee:04a,\n",
      "  author =       {Bo Pang and Lillian Lee},\n",
      "  title =        {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},\n",
      "  booktitle =    \"Proceedings of the ACL\",\n",
      "  year =         2004\n",
      "}\n",
      "\n",
      "=======\n",
      "\n",
      "Data Format Summary \n",
      "\n",
      "- review_polarity.tar.gz: contains this readme and  data used in\n",
      "  the experiments described in Pang/Lee ACL 2004.\n",
      "\n",
      "  Specifically:\n",
      "\n",
      "  Within the folder \"txt_sentoken\" are the 2000 processed down-cased\n",
      "  text files used in Pang/Lee ACL 2004; the names of the two\n",
      "  subdirectories in that folder, \"pos\" and \"neg\", indicate the true\n",
      "  classification (sentiment) of the component files according to our\n",
      "  automatic rating classifier (see section \"Rating Decision\" below).\n",
      "\n",
      "  File names consist of a cross-validation tag plus the name of the\n",
      "  original html file.  The ten folds used in the Pang/Lee ACL 2004 paper's\n",
      "  experiments were:\n",
      "\n",
      "     fold 1: files tagged cv000 through cv099, in numerical order\n",
      "     fold 2: files tagged cv100 through cv199, in numerical order     \n",
      "     ...\n",
      "     fold 10: files tagged cv900 through cv999, in numerical order\n",
      "\n",
      "  Hence, the file neg/cv114_19501.txt, for example, was labeled as\n",
      "  negative, served as a member of fold 2, and was extracted from the\n",
      "  file 19501.html in polarity_html.zip (see below).\n",
      "\n",
      "  Each line in each text file corresponds to a single sentence, as\n",
      "  determined by Adwait Ratnaparkhi's sentence boundary detector\n",
      "  MXTERMINATOR.\n",
      " \n",
      "  Preliminary steps were taken to remove rating information from the\n",
      "  text files, but only the rating information upon which the rating\n",
      "  decision was based is guaranteed to have been removed. Thus, if the\n",
      "  original review contains several instances of rating information,\n",
      "  potentially given in different forms, those not recognized as valid\n",
      "  ratings remain part of the review text.\n",
      "\t\n",
      "- polarity_html.zip: The original source files from which the\n",
      "  processed, labeled, and (randomly) selected data in\n",
      "  review_polarity.tar.gz was derived.\n",
      "\n",
      "  Specifically:  \n",
      "\n",
      "  This data consists of unprocessed, unlabeled html files from the\n",
      "  IMDb archive of the rec.arts.movies.reviews newsgroup,\n",
      "  http://reviews.imdb.com/Reviews. The files in review_polarity.tar.gz\n",
      "  represent a processed subset of these files. \n",
      "\n",
      "=======\n",
      "\n",
      "Rating Decision (Appendix A)\n",
      "\n",
      "This section describes how we determined whether a review was positive\n",
      "or negative.\n",
      "\n",
      "The original html files do not have consistent formats -- a review may\n",
      "not have the author's rating with it, and when it does, the rating can\n",
      "appear at different places in the file in different forms.  We only\n",
      "recognize some of the more explicit ratings, which are extracted via a\n",
      "set of ad-hoc rules.  In essence, a file's classification is determined\n",
      "based on the first rating we were able to identify.\n",
      "\n",
      "\n",
      "- In order to obtain more accurate rating decisions, the maximum\n",
      "\trating must be specified explicitly, both for numerical ratings\n",
      "\tand star ratings.  (\"8/10\", \"four out of five\", and \"OUT OF\n",
      "\t****: ***\" are examples of rating indications we recognize.)\n",
      "\n",
      "- With a five-star system (or compatible number systems):\n",
      "\tthree-and-a-half stars and up are considered positive, \n",
      "\ttwo stars and below are considered negative.\n",
      "- With a four-star system (or compatible number system):\n",
      "\tthree stars and up are considered positive, \n",
      "\tone-and-a-half stars and below are considered negative.  \n",
      "- With a letter grade system:\n",
      "\tB or above is considered positive,\n",
      "\tC- or below is considered negative.\n",
      "\n",
      "We attempted to recognize half stars, but they are specified in an\n",
      "especially free way, which makes them difficult to recognize.  Hence,\n",
      "we may lose a half star very occasionally; but this only results in 2.5\n",
      "stars in five star system being categorized as negative, which is \n",
      "still reasonable.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# credits and infos\n",
    "print(nltk.corpus.movie_reviews.readme())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# how many reviews are we working with?\n",
    "print(len(nltk.corpus.movie_reviews.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791.91 words per review on average.\n"
     ]
    }
   ],
   "source": [
    "# they tend to be quite long\n",
    "print(np.average([len(nltk.corpus.movie_reviews.words(fid)) for fid in nltk.corpus.movie_reviews.fileids()]), \n",
    "      \"words per review on average.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "# all reviews are assumed to be 'opinionated', ie fall under either 'positive' or 'negative' labels\n",
    "print(nltk.corpus.movie_reviews.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "films adapted from comic books have had plenty of success , whether they ' re about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there ' s never really been a comic book like from hell before . for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid ' 80s with a 12 - part series called the watchmen . to say moore and campbell\n",
      "\n",
      "[...]\n",
      "\n",
      ", with the dreamy depp turning in a typically strong performance and deftly handling a british accent . ians holm ( joe gould ' s secret ) and richardson ( 102 dalmatians ) log in great supporting roles , but the big surprise here is graham . i cringed the first time she opened her mouth , imagining her attempt at an irish accent , but it actually wasn ' t half bad . the film , however , is all good . 2 : 00 - r for strong violence / gore , sexuality , language and drug content\n"
     ]
    }
   ],
   "source": [
    "# example of a positive review\n",
    "print(\" \".join(nltk.corpus.movie_reviews.words(nltk.corpus.movie_reviews.fileids(\"pos\")[0])[:100]))\n",
    "print(\"\\n[...]\\n\")\n",
    "print(\" \".join(nltk.corpus.movie_reviews.words(nltk.corpus.movie_reviews.fileids(\"pos\")[0])[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot : two teen couples go to a church party , drink and then drive . they get into an accident . one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . what ' s the deal ? watch the movie and \" sorta \" find out . . . critique : a mind - fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . which is what makes this review an even harder one to\n",
      "\n",
      "[...]\n",
      "\n",
      "also wrapped production two years ago and has been sitting on the shelves ever since . whatever . . . skip it ! where ' s joblo coming from ? a nightmare of elm street 3 ( 7 / 10 ) - blair witch 2 ( 7 / 10 ) - the crow ( 9 / 10 ) - the crow : salvation ( 4 / 10 ) - lost highway ( 10 / 10 ) - memento ( 10 / 10 ) - the others ( 9 / 10 ) - stir of echoes ( 8 / 10 )\n"
     ]
    }
   ],
   "source": [
    "# example of a negative review\n",
    "print(\" \".join(nltk.corpus.movie_reviews.words(nltk.corpus.movie_reviews.fileids(\"neg\")[0])[:100]))\n",
    "print(\"\\n[...]\\n\")\n",
    "print(\" \".join(nltk.corpus.movie_reviews.words(nltk.corpus.movie_reviews.fileids(\"neg\")[0])[-100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process our reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our documents are already sentence-splitted and tokenized, so that we will:\n",
    "\n",
    "- lemmatize and PoS tag the words in each document\n",
    "\n",
    "\n",
    "- remove punctuation and remove stopwords (we use `nltk.corpus.stopwords.words('english')`)\n",
    "\n",
    "\n",
    "- split our documents into a training (80%) and a test (20%) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assign 20%  of our documents to the test set\n",
    "test_ids = set()\n",
    "\n",
    "for polarity in [\"pos\", \"neg\"]:\n",
    "    test_ids.update(random.sample(nltk.corpus.movie_reviews.fileids(polarity), \\\n",
    "                                  int(0.2 * len(nltk.corpus.movie_reviews.fileids(polarity)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_docs = dict((k,[]) for k in [\"pos\", \"neg\"])\n",
    "test_docs = dict((k,[]) for k in [\"pos\", \"neg\"])\n",
    "\n",
    "un2wn_mapping = {\"VERB\" : wn.VERB, \"NOUN\" : wn.NOUN, \"ADJ\" : wn.ADJ, \"ADV\" : wn.ADV}\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# This might take a while, mainly because of the NLTK pos_tag call(s)\n",
    "for polarity in [\"pos\", \"neg\"]:\n",
    "    for did in nltk.corpus.movie_reviews.fileids(polarity):\n",
    "        pdoc = []\n",
    "        for w, p in nltk.pos_tag(nltk.corpus.movie_reviews.words(did), tagset=\"universal\"):\n",
    "            if p in [\".\", \"X\"]:  # filtering\n",
    "                continue\n",
    "            elif w.lower() in stopwords:\n",
    "                if w.lower() in [\"not\", \"t\", \"no\"]:  # to handle negation\n",
    "                    lemma = w.lower()\n",
    "                    p = \"NEGATION\"\n",
    "                else:  # filtering stopwords\n",
    "                    continue\n",
    "            elif p in un2wn_mapping:\n",
    "                lemma = nltk.WordNetLemmatizer().lemmatize(w.lower(), pos = un2wn_mapping[p])\n",
    "            else:\n",
    "                lemma = nltk.WordNetLemmatizer().lemmatize(w.lower())\n",
    "                \n",
    "            pdoc.append(\"-\".join([lemma, p]))\n",
    "            \n",
    "        if did in test_ids:\n",
    "            test_docs[polarity].append(pdoc)\n",
    "        else:\n",
    "            training_docs[polarity].append(pdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['every-DET', 'movie-NOUN', 'come-VERB', 'along-ADV', 'suspect-ADJ', 'studio-NOUN', 'every-DET', 'indication-NOUN', 'stinker-NOUN', 'everybody-NOUN', 'surprise-NOUN', 'perhaps-ADV', 'even-ADV', 'studio-NOUN', 'film-NOUN', 'become-VERB', 'critical-ADJ', 'darling-NOUN', 'mtv-ADJ', 'film-NOUN', \"'-PRT\", '_election-NOUN', 'high-ADJ', 'school-NOUN', 'comedy-NOUN', 'star-VERB', 'matthew-NOUN', 'broderick-NOUN', 'reese-ADJ', 'witherspoon-NOUN', 'current-ADJ', 'example-NOUN', 'anybody-NOUN', 'know-VERB', 'film-NOUN', 'exist-VERB', 'week-NOUN', 'open-VERB', 'plot-NOUN', 'deceptively-ADV', 'simple-ADJ', 'george-VERB', 'washington-ADJ', 'carver-NOUN', 'high-ADJ', 'school-NOUN', 'student-ADJ', 'election-NOUN', 'tracy-NOUN', 'flick-NOUN'] \n",
      "\n",
      "[...]\n",
      "\n",
      "every now and then a movie comes along from a suspect studio , with every indication that it will be a stinker , and to everybody ' s surprise ( perhaps even the studio ) the film becomes a critical darling . mtv films ' _election , a high school comedy starring matthew broderick and reese witherspoon , is a current example . did anybody know this film existed a week before it opened ? the plot is deceptively simple . george washington carver high school is having student elections . tracy flick ( reese witherspoon ) is an over\n"
     ]
    }
   ],
   "source": [
    "# our positive review example from above, lemmatized and PoS tagged (and below, the original for comparison)\n",
    "print(test_docs[\"pos\"][0][:50], \"\\n\\n[...]\\n\")\n",
    "print(\" \".join(nltk.corpus.movie_reviews.words(nltk.corpus.movie_reviews.fileids(\"pos\")[1])[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Techniques: Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the literature, the most reliable features for SC are:\n",
    "\n",
    "\n",
    "- **Terms and their Frequency**: words or n-grams and their frequency. Raw counts, binary features or weighted counts (e.g., with TF-IDF) have been tested.\n",
    "\n",
    "\n",
    "- **Part-of-Speech tags**: soon enough it became evident that some PoS (expecially adjectives) are important indicators of subjectivity and opinion, so they should be treated as special features. \n",
    "\n",
    "\n",
    "- **Opinion words and phrases**: some words or phrases have a strong connotation (i.e., they are regularly used to express a strong sentiment): e.g., *beautiful*, *amazing*, *terrible*. This is true also for noun and verbs (e.g., *crap*, *rubbish*, *hate*), and idioms (e.g. *what a trainwreck!*).\n",
    "\n",
    "\n",
    "- **Syntactic dependency**: the syntactic relations in a sentence may be useful, e.g., to define the scope of an adjective (e.g., *amazing crap*) or of the negation (see below).\n",
    "\n",
    "\n",
    "- **Negation**: negation often reverse the polarity of an expression: \"I don’t like this camera\" is negative. However, negation itself has a scope and this rule has many possible exceptions: e.g., \"not only dumb but also boring\" is a case of negation that does not change the polarity.\n",
    "\n",
    "\n",
    "- **Emoticons**: expecially when working with quickly written, informal text (e.g., tweets, SMSs, etc.) the presence and the polarity of an emoticon is a relevant feature for polarity classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "\n",
    "For this exercise, we consider a simple method that relies solely on the raw frequency of frequent **adjectives, adverbs and negation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's check the frequency of our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate features: 12146\n"
     ]
    }
   ],
   "source": [
    "candidate_features = Counter()\n",
    "\n",
    "for p in [\"pos\", \"neg\"]:\n",
    "    for doc in training_docs[p]:\n",
    "        for w in doc:\n",
    "            if w.split(\"-\")[-1] in [\"ADJ\", \"ADV\", \"NEGATION\"]:\n",
    "                candidate_features[w] += 1\n",
    "\n",
    "print(\"candidate features:\", len(candidate_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1, len(candidate_features) + 1)\n",
    "y = np.array([v for _, v in candidate_features.most_common()])\n",
    "\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.yscale('log') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out all those lemmas that occur less than 10 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected features: 1862\n"
     ]
    }
   ],
   "source": [
    "features = dict()\n",
    "\n",
    "for idx, (f, v) in enumerate(candidate_features.most_common()):\n",
    "    if v == 9:\n",
    "#         print(idx, f, v)\n",
    "        break\n",
    "\n",
    "    features[f] = idx\n",
    "    \n",
    "print(\"selected features:\", len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these features, let's build:\n",
    "\n",
    "- a document (rows) x features (columns) matrix encoding the raw frequencies \n",
    "\n",
    "\n",
    "- a vector encoding, for each document, its polarity (0 = \"negative\", 1 = \"positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fMat = np.zeros((sum([len(v) for v in training_docs.values()]), len(features)))\n",
    "labelsVec = np.zeros((sum([len(v) for v in training_docs.values()])))\n",
    "\n",
    "docId = 0\n",
    "for polarity in [\"neg\", \"pos\"]:\n",
    "    for doc in training_docs[polarity]:\n",
    "        if polarity == \"pos\":\n",
    "            labelsVec[docId] = 1\n",
    "\n",
    "        for word in doc:\n",
    "            if word in features:\n",
    "                fMat[docId, features[word]] += 1\n",
    "\n",
    "        docId += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We experiment with Naive Bayes for multinomially distributed data implemented in the scikit-learn `naive_bayes.MultinomialNB()` class.\n",
    "\n",
    "- This algorithm is suitable for classification with discrete features (e.g., word counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our model by submitting our document/feature matrix and our correct labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(fMat, labelsVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "To evaluate the quality of our model we perform (i.e. **predict**) the annotation of the **test set** (for which we have the correct annotation) and we calculate (reminder!):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**: the percentage of inputs documents correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision, Recall and F-measure** are calculated from the following estimates:\n",
    "\n",
    "- True Positives: items that we correctly identified as positive.\n",
    "- True Negatives: items that we correctly identified as negative.\n",
    "- False Positives (or Type I errors): negative items that we incorrectly identified as positive.\n",
    "- False Negatives (or Type II errors): positive items that we incorrectly identified as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Precision**: how many of the items that we identified were relevant: $\\frac{\\text{TP}}{(\\text{TP}+\\text{FP})}$.\n",
    "\n",
    "\n",
    "- **Recall**: ow many of the relevant items that we identified, is $\\frac{\\text{TP}}{(\\text{TP}+\\text{FN})}$,\n",
    "\n",
    "\n",
    "- **F$_1$-measure**: the harmonic mean of the precision and recall: $2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, extract the features from the test set (and create a vector for the gold standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testMat = np.zeros((sum([len(v) for v in test_docs.values()]), len(features)))\n",
    "goldStandard = np.zeros((sum([len(v) for v in test_docs.values()])))\n",
    "\n",
    "docId = 0\n",
    "for polarity in [\"neg\", \"pos\"]:\n",
    "    for doc in test_docs[polarity]:\n",
    "        if polarity == \"pos\":\n",
    "            goldStandard[docId] = 1\n",
    "\n",
    "        for word in doc:\n",
    "            if word in features:\n",
    "                testMat[docId, features[word]] += 1\n",
    "\n",
    "        docId += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, annotate the test set with our previously trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(testMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, calculate our evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8225\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "print(\"accuracy:\", metrics.accuracy_score(predicted, goldStandard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.785\n",
      "recall: 0.8486486486486486\n",
      "f1-measure: 0.8155844155844156\n"
     ]
    }
   ],
   "source": [
    "# precision, recall and f-measure\n",
    "print(\"precision:\", metrics.precision_score(predicted, goldStandard))\n",
    "\n",
    "print(\"recall:\", metrics.recall_score(predicted, goldStandard))\n",
    "\n",
    "print(\"f1-measure:\", metrics.f1_score(predicted, goldStandard))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question\n",
    "\n",
    "Why are the accuracy and precision results different? Can you cast accuracy in terms of the precision formula above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question\n",
    "\n",
    "See what happens to your classifier if you use term frequency-inverse document frequency (tf-idf) to weight the raw frequencies\n",
    "\n",
    "\n",
    "- Recall that tf-idf is a composite weight for each term $t$ in each document $d$, estimated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{tf-idf}_{t,d} = \\text{tf}_{t,d} \\times \\text{idf}_t = \\text{tf}_{t,d} \\times log \\left( \\frac{N}{\\text{df}_t}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf$_{t,d}$ is the number of occurrence of $t$ in $d$\n",
    "- $N$ is the total number of documents\n",
    "- df$_t$ is the number of documents in which $t$ appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon-based Approaches (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexicon based approaches derive the polarity of a word (or of a phrase) from a sentiment lexicon. In their base form, LB approaches classify a document by summing up all sentiment expressions in the document.\n",
    "\n",
    "\n",
    "- Sentiment lexicon: a collection of known sentiment terms, phrases and even idioms annotated for their sentiment.\n",
    "\n",
    "\n",
    "\n",
    "- These resources can be created **manually** or **automatically**.\n",
    "\n",
    "\n",
    "\n",
    "- In their simplest form, these resources are lists of positive/negative words, but more recent resources tend to associated **gradable scores** to each entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The oldest sentiment lexicon is the [General Inquirer](http://www.wjh.harvard.edu/~inquirer/) (Stone et al., 1966):\n",
    "\n",
    "\n",
    "- drews on work in psycholinguistics (Osgood et al., 1957) and on work in content analysis\n",
    "\n",
    "\n",
    "- lexicon of 1915 positive words and 2291 negative words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [MPQA Subjectivy lexicon](http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/) (Wilson et al., 2005) is derived by combining several resource:\n",
    "\n",
    "- 2718 positive and 4912 negative words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [polarity lexicon](http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar) by Hu and Liu (2004) consisting of lists of strings:\n",
    "\n",
    "- drawn from product reviews, labeled using a bootstrapping method from WordNet\n",
    "\n",
    "\n",
    "- 2006 positive and 4783 negative words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some sample words with consistent sentiment across these three sentiment lexicons** (from Jurafsky & Marin (2018):\n",
    "\n",
    ">**Positive**: `admire, amazing, assure, celebration, charm, eager, enthusiastic, excellent, fancy, fantastic, frolic, graceful, happy, joy, luck, majesty, mercy, nice, patience, perfect, proud, rejoice, relief, respect, satisfactorily, sensational, super, terrific, thank, vivid, wise, wonderful, zest`.\n",
    ">\n",
    ">\n",
    ">**Negative**: `abominable, anger, anxious, bad, catastrophe, cheap, complaint, condescending, deceit, defective, disappointment, embarrass, fake, fear, filthy, fool, guilt, hate, idiot, inflict, lazy, miserable, mourn, nervous, objection, pest, plot, reject, scream, silly, terrible, unfriendly, vile, wicked`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### SentiWordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SentiWordNet](https://github.com/aesuli/sentiwordnet) is a lexical resource in which each WordNet synset is associated to three numerical scores describing how objective, positive, and negative the terms contained in the synset are.\n",
    "\n",
    "- The semi-supervised method used to develop SentiWordNet is based on:\n",
    "\n",
    "    - the manual annotation of two sets of seeds (positive and negative terms) and their expansion on the basis of the synonym and antonymy relations encoded in WordNet;\n",
    "    - the quantitative analysis of the glosses associated to synsets, and on the use of the resulting vectorial term representations for semi-supervised synset classification. The basic assumption is that  terms with similar polarity tend to have \"similar\" glosses.\n",
    "\n",
    "\n",
    "- The three scores are derived by combining the results produced by a committee of eight ternary classifiers, all characterized by similar accuracy levels but different classification behaviour.\n",
    "\n",
    "\n",
    "- Due to the synset-based nature of the WordNet representation, given that this lexical resource provides a synset-based sentiment representation, different senses of the same term may have different sentiment scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving Synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to retrieve synsets is by submitting the relevant lemma to the `senti_synsets()` method, that returns the list of all the synsets containing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/giovannicolavizza/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the sentiwordnet data (only needed once...)\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'), SentiSynset('slow.v.03'), SentiSynset('slow.a.01'), SentiSynset('slow.a.02'), SentiSynset('dense.s.04'), SentiSynset('slow.a.04'), SentiSynset('boring.s.01'), SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'), SentiSynset('behind.r.03')]\n"
     ]
    }
   ],
   "source": [
    "print(list(swn.senti_synsets('slow')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optional paramater `pos` allows you to constrain the search to a given part of speech \n",
    "\n",
    "- available options: `wn.NOUN`, `wn.VERB`, `wn.ADJ`, `wn.ADV` (require wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SentiSynset('happy.a.01'), SentiSynset('felicitous.s.02'), SentiSynset('glad.s.02'), SentiSynset('happy.s.04')]\n"
     ]
    }
   ],
   "source": [
    "print(list(swn.senti_synsets('happy', pos = wn.ADJ)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to access all the synset in the resource, you can call the `all_senti_synsets()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117659"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(swn.all_senti_synsets()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `senti_synset()` method together with the notation `lemma.pos.numer` (e.g. `breakdown.n.03`) to access a given synset (or to refer to its annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<breakdown.n.03: PosScore=0.0 NegScore=0.25>\n"
     ]
    }
   ],
   "source": [
    "breakdown = swn.senti_synset('breakdown.n.03')\n",
    "print(breakdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods `pos_score()`, `neg_score()` and `obj_score()` can be called to retrieve the sentiment scores associated with a given synset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breakdown.pos_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breakdown.neg_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breakdown.obj_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question\n",
    "\n",
    "Use the following simple strategy:\n",
    "\n",
    "- for each word in a document, retrieve all the synsets available in SentiWordNet;\n",
    "\n",
    "\n",
    "- average across all the senses that are associated with a given lemma;\n",
    "\n",
    "\n",
    "- calculate the polarity of the whole document as the sum of the polarity of its words;\n",
    "\n",
    "\n",
    "- see how things change if you filter out the words with a low subjective score;\n",
    "\n",
    "\n",
    "- compare with the Naive Bayes method in terms of accuracy, precision, recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
